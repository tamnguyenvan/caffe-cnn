{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection\n",
    "This notebook contains code for State Farm Distracted Driver Detection dataset chanllenge. Kaggle link: https://www.kaggle.com/c/state-farm-distracted-driver-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.5/dist-packages (20.3.4)\n",
      "Collecting pip\n",
      "  Using cached pip-20.3.4-py2.py3-none-any.whl (1.5 MB)\n",
      "  Using cached pip-20.3.3-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages (4.56.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.5/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (0.22.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2018.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.14.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib) (39.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tqdm matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import caffe\n",
    "import lmdb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from caffe.proto import caffe_pb2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "For large dataset we usually split the dataset into 3 subsets: training, validation and testing. We already have a individual test set for final evaluation. So we still need to split the original training set into a training and a validation set for parameters tuning.\n",
    "\n",
    "As you see the datas are already seperated into each class: one directory for images with label c0, another directory for images with label c1, etc. For loading the dataset thus we don't need the .csv file provided, we can go to the directories one by one and load the images, and for each image we record the parent directory name (c0, c1, ...) as the label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_dir, split=0.2):\n",
    "    \"\"\"Load raw data and split it into training and validation subset.\n",
    "    \n",
    "    Args\n",
    "    :data_dir: Data root directory.\n",
    "    \n",
    "    Returns\n",
    "    :X_train: A list of training image paths.\n",
    "    :y_train: A list of training labels.\n",
    "    :X_val: A list of validation image paths.\n",
    "    :y_val: A list of validation labels.\n",
    "    \"\"\"\n",
    "    imgs_list = []\n",
    "    labels = []\n",
    "\n",
    "    # List all image subdirectories and sort by class name\n",
    "    img_dirs = sorted(glob.glob(os.path.join(data_dir, '*')), key = lambda k: k.split(\"/\")[-1])\n",
    "    for img_dir in img_dirs:\n",
    "        # Read all the images in this class\n",
    "        # Image subdirectory name as label\n",
    "        for img_path in glob.glob(os.path.join(img_dir,'*.jpg')):\n",
    "            imgs_list.append(img_path)\n",
    "            labels.append(int(img_dir.split(\"/\")[-1].replace('c', '')))\n",
    "    \n",
    "    # Split into training and validation subset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(imgs_list, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "    return np.array(X_train), np.array(X_test), y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "Here we use the function we just defined above to load data. Make sure you placed the dataset in docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_images = 'imgs/train'\n",
    "path_test_images = 'imgs/test'\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data(path_train_images)\n",
    "\n",
    "print('Size of X_train: {}, size of y_train: {}'.format(len(X_train), len(y_train)))\n",
    "print('Size of X_test: {}, size of y_test: {}'.format(len(X_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sanity check\n",
    "\n",
    "Classes:\n",
    "- c0: safe driving\n",
    "- c1: texting - right\n",
    "- c2: talking on the phone - right\n",
    "- c3: texting - left\n",
    "- c4: talking on the phone - left\n",
    "- c5: operating the radio\n",
    "- c6: drinking\n",
    "- c7: reaching behind\n",
    "- c8: hair and makeup\n",
    "- c9: talking to passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = cv2.imread(X_train[0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Check label\n",
    "print('Class: {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploring\n",
    "It's common in Machine Learning to see how balanced the dataset is. It's the best if our dataset is perfectly balanced. Otherwise we need to apply techniques to address it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_label_csv = 'driver_imgs_list.csv'\n",
    "data_file = pd.read_csv(path_label_csv)\n",
    "data_x = list(pd.unique(data_file['classname']))\n",
    "\n",
    "# Clustring all images of each class together\n",
    "data_classes = data_file.loc[:, ['classname','img']].groupby(by='classname').count().reset_index()\n",
    "data_y =list(data_classes['img'])\n",
    "\n",
    "# Plotting them using matplot\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.bar(data_x, data_y)\n",
    "plt.ylabel('Count classes')\n",
    "plt.title('Classes')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky us! The dataset is very nice. All classes are nearly equal in quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caffe Overview\n",
    "Caffe is a deep learning framework developed by the Berkeley Vision and Learning Center (BVLC). It is written in C++ and has Python and Matlab interfaces.\n",
    "\n",
    "There are 4 steps in training a CNN using Caffe:\n",
    "\n",
    "- Step 1 - Data preparation: In this step, we get the images and store them in a format that can be used by Caffe. Here we will write a Python script that will handle image storage.\n",
    "\n",
    "- Step 2 - Model definition: In this step, we choose a CNN architecture and we define its parameters in a configuration file with extension .prototxt.\n",
    "\n",
    "- Step 3 - Solver definition: The solver is responsible for model optimization. We define the solver parameters in a configuration file with extension .prototxt.\n",
    "\n",
    "- Step 4 - Model training: We train the model by executing caffe command from the terminal. After training the model, we will get the trained model in a file with extension .caffemodel.\n",
    "\n",
    "After the training phase, we will use the .caffemodel trained model to make predictions of new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Here we prepare the raw dataset as LMDB database, which is standard Caffe data format. We need some piece of Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "\n",
    "def transform_img(img, img_width=128, img_height=128):\n",
    "    \"\"\"Resize and normalize image.\n",
    "    \n",
    "    Args\n",
    "    :img: numpy array image.\n",
    "    :img_width: Target image width.\n",
    "    :img_height: Target image height.\n",
    "    \n",
    "    Returns\n",
    "      transformed image.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, (img_width, img_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # TODO: normalize\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_datum(img, label):\n",
    "    \"\"\"\n",
    "    Convert original numpy array image to datum\n",
    "    Args\n",
    "    :img: numpy.ndarray (BGR instead of RGB)\n",
    "    :label: int\n",
    "    \"\"\"\n",
    "    return caffe_pb2.Datum(\n",
    "        channels=3,\n",
    "        width=128,\n",
    "        height=128,\n",
    "        label=label,\n",
    "        data=np.rollaxis(img, 2).tostring())\n",
    "\n",
    "\n",
    "def make_lmdb(lmdb_path, x_data, y_data):\n",
    "    \"\"\"Create LMDB database from the given raw images and labels.\n",
    "    \n",
    "    Args\n",
    "    :lmdb_path: LMDB output path.\n",
    "    :x_data: A list of image paths.\n",
    "    :y_data: A list of labels.\n",
    "    \"\"\"\n",
    "    in_db = lmdb.open(lmdb_path, map_size=int(1e12))\n",
    "    with in_db.begin(write=True) as in_txn:\n",
    "        for in_idx, img_path in tqdm.tqdm(enumerate(x_data)):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = transform_img(img)\n",
    "            datum = make_datum(img, y_data[in_idx])  # Making datum object\n",
    "            in_txn.put('{:0>5d}'.format(in_idx).encode('utf-8'), datum.SerializeToString())\n",
    "    in_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually create training and validation database\n",
    "train_lmdb = 'input/train_lmdb'\n",
    "val_lmdb = 'input/validation_lmdb'\n",
    "\n",
    "os.makedirs(train_lmdb, exist_ok=True)\n",
    "os.makedirs(val_lmdb, exist_ok=True)\n",
    "make_lmdb(train_lmdb, X_train, y_train)\n",
    "make_lmdb(val_lmdb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create architecture\n",
    "Caffe philosophy is expressivity and speed. For that we use text files to define networks, instead of code API like Keras. Coding is possible in Caffe too, but highly discoureged.\n",
    "\n",
    "After deciding on the CNN architecture, we need to define its parameters in a .prototxt file. Here is the details of the defined network structure in my git repo.\n",
    "\n",
    "### 1. Data Layer\n",
    "Data enters Caffe through data layers: they lie at the bottom of nets. Data can come from efficient databases (LevelDB or LMDB), directly from memory, or, when efficiency is not critical, from files on disk in HDF5 or common image formats. Parameters we have in data layer:\n",
    "\n",
    "- source: the path to the datas it needs to read\n",
    "- backend: specifies the data type that we read\n",
    "- batch_size: specifies the size of image batches to read at each step\n",
    "- transform_param: input transformation params. In computer vision task, it's common to normalize input images. Here we provide 3 mean values correspond to 3 image channels. `mirror` means horizontal flip augmentation.\n",
    "```\n",
    "layer {\n",
    "  name: “data”\n",
    "  type: “Data”\n",
    "  include {\n",
    "    phase: TRAIN   # Or TEST\n",
    "  }\n",
    "  data_param {\n",
    "    source: \"./input/train_lmdb\"\n",
    "    backend: LMDB\n",
    "    batch_size: 32\n",
    "  }\n",
    "  top: “data”\n",
    "  top: “label”\n",
    "  transform_param {\n",
    "    crop_size: 224\n",
    "    mean_value: 104\n",
    "    mean_value: 117\n",
    "    mean_value: 123\n",
    "    mirror: true\n",
    " }\n",
    "}\n",
    "```\n",
    "### 2. Convolution layer\n",
    "This layer recieves the data blob from last layer and produces conv1 blob. Convolution layers in neural networks generally convolve the input image with a set of learnable filters, each producing one feature map in the output image.\n",
    "\n",
    "This layer produces 64 filters and kernel size is 3 with the stride of 1 done on input. Fillers help us initialize weight and bias values randomly. Here we use Xavier algorithm to automatically initialize weights based on the number of input and output neurons. And for bias we use a simple constant number of zero. lr_mult is also the settings for learning rate, here we set the learning rate for weights same as the resolver in runtime and the learning rate for bias twice of that.\n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  param { lr_mult: 1 }\n",
    "  param { lr_mult: 2 }\n",
    "  convolution_param {\n",
    "    num_output: 64\n",
    "    kernel_size: 3\n",
    "    stride: 1\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "  bottom: \"data\"\n",
    "  top: \"conv1\"\n",
    "}\n",
    "```\n",
    "### 3. Pooling layer\n",
    "We set the pool to max so it does max pooling operation on convolution outputs.\n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  pooling_param {\n",
    "    kernel_size: 2\n",
    "    stride: 2\n",
    "    pool: MAX\n",
    "  }\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "}\n",
    "```\n",
    "### 4. Dense layer\n",
    "This layer is similar to previous layers too. Dense layers are knows as InnerProduct layers in Caffe. Here we have a dense layer which has 128 output and parameters is same as previous layers explained.\n",
    "\n",
    "```\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  param { lr_mult: 1 }\n",
    "  param { lr_mult: 2 }\n",
    "  inner_product_param {\n",
    "    num_output: 128\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "```\n",
    "### 5. ReLU layer\n",
    "Since ReLU is element-wise we can do the operation once and not waste memory. This can be done with defining one name for top and bottom layers. Note that we can not have same names for blob of other layers and this is pecuilar for this layer.\n",
    "```\n",
    "layer {\n",
    "  name: \"relu1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip1\"\n",
    "  top: \"ip1\"\n",
    "}\n",
    "```\n",
    "\n",
    "After ReLU we define another Dense layer with bottom: \"ip1\" and top: \"ip2\"\n",
    "\n",
    "### 6. Loss\n",
    "We define loss as follow:\n",
    "```\n",
    "layer {\n",
    "  name: \"loss\"\n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  bottom: \"ip2\"\n",
    "  bottom: \"label\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caffe Solver\n",
    "The solver is responsible for model optimization. We define the solver's parameters in a .prototxt file. \n",
    "\n",
    "This solver computes the accuracy of the model using the validation set every 1000 iterations. The optimization process will run for a maximum of 40000 iterations and will take a snapshot of the trained model every 5000 iterations.\n",
    "\n",
    "base_lr, lr_policy, gamma, momentum and weight_decay are hyperparameters that we need to tune to get a good convergence of the model.\n",
    "\n",
    "we chose lr_policy: \"step\" with stepsize: 2500, base_lr: 0.001 and gamma: 0.1. In this configuration, we will start with a learning rate of 0.001, and we will drop the learning rate by a factor of ten every 2500 iterations.\n",
    "\n",
    "There are different strategies for the optimization process. For a detailed explanation, you can read Caffe's solver documentation.\n",
    "```\n",
    "net: \"caffe-cnn/cnn/cnn.prototxt\"\n",
    "test_interval: 1000\n",
    "base_lr: 0.001\n",
    "lr_policy: \"step\"\n",
    "gamma: 0.1\n",
    "stepsize: 2500\n",
    "display: 50\n",
    "max_iter: 40000\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "snapshot: 5000\n",
    "snapshot_prefix: \"./snapshot/cnn\"\n",
    "solver_mode: CPU\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We defined the network archirecture and solver in git repo. So just clone it and run the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'caffe-cnn'...\n",
      "remote: Enumerating objects: 201, done.\u001b[K\n",
      "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
      "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
      "remote: Total 201 (delta 94), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (201/201), 213.37 KiB | 263.00 KiB/s, done.\n",
      "Resolving deltas: 100% (94/94), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tamnguyenvan/caffe-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:17:32.396397   108 upgrade_proto.cpp:1113] snapshot_prefix was a directory and is replaced to ./snapshot/cnn/cnn_solver\r\n",
      "I0124 08:17:32.396842   108 caffe.cpp:197] Use CPU.\r\n",
      "I0124 08:17:32.397048   108 solver.cpp:45] Initializing solver from parameters: \r\n",
      "test_iter: 141\r\n",
      "test_interval: 1000\r\n",
      "base_lr: 0.001\r\n",
      "display: 50\r\n",
      "max_iter: 40000\r\n",
      "lr_policy: \"step\"\r\n",
      "gamma: 0.1\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "stepsize: 2500\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"./snapshot/cnn/cnn_solver\"\r\n",
      "solver_mode: CPU\r\n",
      "net: \"caffe-cnn/cnn/cnn.prototxt\"\r\n",
      "train_state {\r\n",
      "  level: 0\r\n",
      "  stage: \"\"\r\n",
      "}\r\n",
      "I0124 08:17:32.397346   108 solver.cpp:102] Creating training net from net file: caffe-cnn/cnn/cnn.prototxt\r\n",
      "I0124 08:17:32.397657   108 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\r\n",
      "I0124 08:17:32.397697   108 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\r\n",
      "I0124 08:17:32.397855   108 net.cpp:51] Initializing net from parameters: \r\n",
      "name: \"CNN\"\r\n",
      "state {\r\n",
      "  phase: TRAIN\r\n",
      "  level: 0\r\n",
      "  stage: \"\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TRAIN\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    mirror: true\r\n",
      "    crop_size: 128\r\n",
      "    mean_value: 104\r\n",
      "    mean_value: 117\r\n",
      "    mean_value: 123\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./input/train_lmdb\"\r\n",
      "    batch_size: 32\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 32\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"conv1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 64\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu2\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"conv2\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv3\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"conv3\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 128\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu3\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"conv3\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool3\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"pool3\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv4\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool3\"\r\n",
      "  top: \"conv4\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 128\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu4\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"conv4\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool4\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"pool4\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc5\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool4\"\r\n",
      "  top: \"fc5\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 512\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu5\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc5\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"drop5\"\r\n",
      "  type: \"Dropout\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc5\"\r\n",
      "  dropout_param {\r\n",
      "    dropout_ratio: 0.5\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc6\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc6\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"fc6\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "I0124 08:17:32.399556   108 layer_factory.hpp:77] Creating layer data\r\n",
      "I0124 08:17:32.399693   108 db_lmdb.cpp:35] Opened lmdb ./input/train_lmdb\r\n",
      "I0124 08:17:32.399745   108 net.cpp:84] Creating Layer data\r\n",
      "I0124 08:17:32.399771   108 net.cpp:380] data -> data\r\n",
      "I0124 08:17:32.399807   108 net.cpp:380] data -> label\r\n",
      "I0124 08:17:32.400104   108 data_layer.cpp:45] output data size: 32,3,128,128\r\n",
      "I0124 08:17:32.400216   108 net.cpp:122] Setting up data\r\n",
      "I0124 08:17:32.400245   108 net.cpp:129] Top shape: 32 3 128 128 (1572864)\r\n",
      "I0124 08:17:32.400264   108 net.cpp:129] Top shape: 32 (32)\r\n",
      "I0124 08:17:32.400300   108 net.cpp:137] Memory required for data: 6291584\r\n",
      "I0124 08:17:32.400321   108 layer_factory.hpp:77] Creating layer conv1\r\n",
      "I0124 08:17:32.400348   108 net.cpp:84] Creating Layer conv1\r\n",
      "I0124 08:17:32.400369   108 net.cpp:406] conv1 <- data\r\n",
      "I0124 08:17:32.400393   108 net.cpp:380] conv1 -> conv1\r\n",
      "I0124 08:17:32.400460   108 net.cpp:122] Setting up conv1\r\n",
      "I0124 08:17:32.400485   108 net.cpp:129] Top shape: 32 32 128 128 (16777216)\r\n",
      "I0124 08:17:32.400501   108 net.cpp:137] Memory required for data: 73400448\r\n",
      "I0124 08:17:32.400530   108 layer_factory.hpp:77] Creating layer relu1\r\n",
      "I0124 08:17:32.400552   108 net.cpp:84] Creating Layer relu1\r\n",
      "I0124 08:17:32.400570   108 net.cpp:406] relu1 <- conv1\r\n",
      "I0124 08:17:32.400589   108 net.cpp:367] relu1 -> conv1 (in-place)\r\n",
      "I0124 08:17:32.400610   108 net.cpp:122] Setting up relu1\r\n",
      "I0124 08:17:32.400629   108 net.cpp:129] Top shape: 32 32 128 128 (16777216)\r\n",
      "I0124 08:17:32.400647   108 net.cpp:137] Memory required for data: 140509312\r\n",
      "I0124 08:17:32.400663   108 layer_factory.hpp:77] Creating layer pool1\r\n",
      "I0124 08:17:32.400682   108 net.cpp:84] Creating Layer pool1\r\n",
      "I0124 08:17:32.400698   108 net.cpp:406] pool1 <- conv1\r\n",
      "I0124 08:17:32.400718   108 net.cpp:380] pool1 -> pool1\r\n",
      "I0124 08:17:32.400753   108 net.cpp:122] Setting up pool1\r\n",
      "I0124 08:17:32.400774   108 net.cpp:129] Top shape: 32 32 64 64 (4194304)\r\n",
      "I0124 08:17:32.400790   108 net.cpp:137] Memory required for data: 157286528\r\n",
      "I0124 08:17:32.400806   108 layer_factory.hpp:77] Creating layer conv2\r\n",
      "I0124 08:17:32.400830   108 net.cpp:84] Creating Layer conv2\r\n",
      "I0124 08:17:32.400846   108 net.cpp:406] conv2 <- pool1\r\n",
      "I0124 08:17:32.400866   108 net.cpp:380] conv2 -> conv2\r\n",
      "I0124 08:17:32.400949   108 net.cpp:122] Setting up conv2\r\n",
      "I0124 08:17:32.400974   108 net.cpp:129] Top shape: 32 64 64 64 (8388608)\r\n",
      "I0124 08:17:32.400990   108 net.cpp:137] Memory required for data: 190840960\r\n",
      "I0124 08:17:32.401012   108 layer_factory.hpp:77] Creating layer relu2\r\n",
      "I0124 08:17:32.401031   108 net.cpp:84] Creating Layer relu2\r\n",
      "I0124 08:17:32.401049   108 net.cpp:406] relu2 <- conv2\r\n",
      "I0124 08:17:32.401067   108 net.cpp:367] relu2 -> conv2 (in-place)\r\n",
      "I0124 08:17:32.401087   108 net.cpp:122] Setting up relu2\r\n",
      "I0124 08:17:32.401105   108 net.cpp:129] Top shape: 32 64 64 64 (8388608)\r\n",
      "I0124 08:17:32.401124   108 net.cpp:137] Memory required for data: 224395392\r\n",
      "I0124 08:17:32.401140   108 layer_factory.hpp:77] Creating layer pool2\r\n",
      "I0124 08:17:32.401158   108 net.cpp:84] Creating Layer pool2\r\n",
      "I0124 08:17:32.401175   108 net.cpp:406] pool2 <- conv2\r\n",
      "I0124 08:17:32.401194   108 net.cpp:380] pool2 -> pool2\r\n",
      "I0124 08:17:32.401216   108 net.cpp:122] Setting up pool2\r\n",
      "I0124 08:17:32.401234   108 net.cpp:129] Top shape: 32 64 32 32 (2097152)\r\n",
      "I0124 08:17:32.401252   108 net.cpp:137] Memory required for data: 232784000\r\n",
      "I0124 08:17:32.401269   108 layer_factory.hpp:77] Creating layer conv3\r\n",
      "I0124 08:17:32.401289   108 net.cpp:84] Creating Layer conv3\r\n",
      "I0124 08:17:32.401305   108 net.cpp:406] conv3 <- pool2\r\n",
      "I0124 08:17:32.401324   108 net.cpp:380] conv3 -> conv3\r\n",
      "I0124 08:17:32.401553   108 net.cpp:122] Setting up conv3\r\n",
      "I0124 08:17:32.401577   108 net.cpp:129] Top shape: 32 128 32 32 (4194304)\r\n",
      "I0124 08:17:32.401594   108 net.cpp:137] Memory required for data: 249561216\r\n",
      "I0124 08:17:32.401618   108 layer_factory.hpp:77] Creating layer relu3\r\n",
      "I0124 08:17:32.401643   108 net.cpp:84] Creating Layer relu3\r\n",
      "I0124 08:17:32.401660   108 net.cpp:406] relu3 <- conv3\r\n",
      "I0124 08:17:32.401679   108 net.cpp:367] relu3 -> conv3 (in-place)\r\n",
      "I0124 08:17:32.401698   108 net.cpp:122] Setting up relu3\r\n",
      "I0124 08:17:32.401718   108 net.cpp:129] Top shape: 32 128 32 32 (4194304)\r\n",
      "I0124 08:17:32.401734   108 net.cpp:137] Memory required for data: 266338432\r\n",
      "I0124 08:17:32.401751   108 layer_factory.hpp:77] Creating layer pool3\r\n",
      "I0124 08:17:32.401769   108 net.cpp:84] Creating Layer pool3\r\n",
      "I0124 08:17:32.401787   108 net.cpp:406] pool3 <- conv3\r\n",
      "I0124 08:17:32.401808   108 net.cpp:380] pool3 -> pool3\r\n",
      "I0124 08:17:32.401830   108 net.cpp:122] Setting up pool3\r\n",
      "I0124 08:17:32.401849   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.401881   108 net.cpp:137] Memory required for data: 270532736\r\n",
      "I0124 08:17:32.401899   108 layer_factory.hpp:77] Creating layer conv4\r\n",
      "I0124 08:17:32.401919   108 net.cpp:84] Creating Layer conv4\r\n",
      "I0124 08:17:32.401935   108 net.cpp:406] conv4 <- pool3\r\n",
      "I0124 08:17:32.401957   108 net.cpp:380] conv4 -> conv4\r\n",
      "I0124 08:17:32.402374   108 net.cpp:122] Setting up conv4\r\n",
      "I0124 08:17:32.402398   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.402415   108 net.cpp:137] Memory required for data: 274727040\r\n",
      "I0124 08:17:32.402434   108 layer_factory.hpp:77] Creating layer relu4\r\n",
      "I0124 08:17:32.402454   108 net.cpp:84] Creating Layer relu4\r\n",
      "I0124 08:17:32.402472   108 net.cpp:406] relu4 <- conv4\r\n",
      "I0124 08:17:32.402490   108 net.cpp:367] relu4 -> conv4 (in-place)\r\n",
      "I0124 08:17:32.402509   108 net.cpp:122] Setting up relu4\r\n",
      "I0124 08:17:32.402529   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.402544   108 net.cpp:137] Memory required for data: 278921344\r\n",
      "I0124 08:17:32.402560   108 layer_factory.hpp:77] Creating layer pool4\r\n",
      "I0124 08:17:32.402581   108 net.cpp:84] Creating Layer pool4\r\n",
      "I0124 08:17:32.402601   108 net.cpp:406] pool4 <- conv4\r\n",
      "I0124 08:17:32.402619   108 net.cpp:380] pool4 -> pool4\r\n",
      "I0124 08:17:32.402640   108 net.cpp:122] Setting up pool4\r\n",
      "I0124 08:17:32.402662   108 net.cpp:129] Top shape: 32 128 8 8 (262144)\r\n",
      "I0124 08:17:32.402681   108 net.cpp:137] Memory required for data: 279969920\r\n",
      "I0124 08:17:32.402698   108 layer_factory.hpp:77] Creating layer fc5\r\n",
      "I0124 08:17:32.402717   108 net.cpp:84] Creating Layer fc5\r\n",
      "I0124 08:17:32.402734   108 net.cpp:406] fc5 <- pool4\r\n",
      "I0124 08:17:32.402757   108 net.cpp:380] fc5 -> fc5\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:17:32.513347   108 net.cpp:122] Setting up fc5\r\n",
      "I0124 08:17:32.513515   108 net.cpp:129] Top shape: 32 512 (16384)\r\n",
      "I0124 08:17:32.513536   108 net.cpp:137] Memory required for data: 280035456\r\n",
      "I0124 08:17:32.513566   108 layer_factory.hpp:77] Creating layer relu5\r\n",
      "I0124 08:17:32.513604   108 net.cpp:84] Creating Layer relu5\r\n",
      "I0124 08:17:32.513622   108 net.cpp:406] relu5 <- fc5\r\n",
      "I0124 08:17:32.513762   108 net.cpp:367] relu5 -> fc5 (in-place)\r\n",
      "I0124 08:17:32.513790   108 net.cpp:122] Setting up relu5\r\n",
      "I0124 08:17:32.513808   108 net.cpp:129] Top shape: 32 512 (16384)\r\n",
      "I0124 08:17:32.513821   108 net.cpp:137] Memory required for data: 280100992\r\n",
      "I0124 08:17:32.513836   108 layer_factory.hpp:77] Creating layer drop5\r\n",
      "I0124 08:17:32.513855   108 net.cpp:84] Creating Layer drop5\r\n",
      "I0124 08:17:32.513870   108 net.cpp:406] drop5 <- fc5\r\n",
      "I0124 08:17:32.513963   108 net.cpp:367] drop5 -> fc5 (in-place)\r\n",
      "I0124 08:17:32.514000   108 net.cpp:122] Setting up drop5\r\n",
      "I0124 08:17:32.514019   108 net.cpp:129] Top shape: 32 512 (16384)\r\n",
      "I0124 08:17:32.514034   108 net.cpp:137] Memory required for data: 280166528\r\n",
      "I0124 08:17:32.514048   108 layer_factory.hpp:77] Creating layer fc6\r\n",
      "I0124 08:17:32.514075   108 net.cpp:84] Creating Layer fc6\r\n",
      "I0124 08:17:32.514092   108 net.cpp:406] fc6 <- fc5\r\n",
      "I0124 08:17:32.514175   108 net.cpp:380] fc6 -> fc6\r\n",
      "I0124 08:17:32.514214   108 net.cpp:122] Setting up fc6\r\n",
      "I0124 08:17:32.514238   108 net.cpp:129] Top shape: 32 10 (320)\r\n",
      "I0124 08:17:32.514256   108 net.cpp:137] Memory required for data: 280167808\r\n",
      "I0124 08:17:32.514276   108 layer_factory.hpp:77] Creating layer loss\r\n",
      "I0124 08:17:32.514297   108 net.cpp:84] Creating Layer loss\r\n",
      "I0124 08:17:32.514312   108 net.cpp:406] loss <- fc6\r\n",
      "I0124 08:17:32.514394   108 net.cpp:406] loss <- label\r\n",
      "I0124 08:17:32.514417   108 net.cpp:380] loss -> loss\r\n",
      "I0124 08:17:32.514446   108 layer_factory.hpp:77] Creating layer loss\r\n",
      "I0124 08:17:32.514479   108 net.cpp:122] Setting up loss\r\n",
      "I0124 08:17:32.514495   108 net.cpp:129] Top shape: (1)\r\n",
      "I0124 08:17:32.514508   108 net.cpp:132]     with loss weight 1\r\n",
      "I0124 08:17:32.514614   108 net.cpp:137] Memory required for data: 280167812\r\n",
      "I0124 08:17:32.514634   108 net.cpp:198] loss needs backward computation.\r\n",
      "I0124 08:17:32.514652   108 net.cpp:198] fc6 needs backward computation.\r\n",
      "I0124 08:17:32.514663   108 net.cpp:198] drop5 needs backward computation.\r\n",
      "I0124 08:17:32.514676   108 net.cpp:198] relu5 needs backward computation.\r\n",
      "I0124 08:17:32.514715   108 net.cpp:198] fc5 needs backward computation.\r\n",
      "I0124 08:17:32.514796   108 net.cpp:198] pool4 needs backward computation.\r\n",
      "I0124 08:17:32.514817   108 net.cpp:198] relu4 needs backward computation.\r\n",
      "I0124 08:17:32.514830   108 net.cpp:198] conv4 needs backward computation.\r\n",
      "I0124 08:17:32.514843   108 net.cpp:198] pool3 needs backward computation.\r\n",
      "I0124 08:17:32.514855   108 net.cpp:198] relu3 needs backward computation.\r\n",
      "I0124 08:17:32.514866   108 net.cpp:198] conv3 needs backward computation.\r\n",
      "I0124 08:17:32.514879   108 net.cpp:198] pool2 needs backward computation.\r\n",
      "I0124 08:17:32.514894   108 net.cpp:198] relu2 needs backward computation.\r\n",
      "I0124 08:17:32.514917   108 net.cpp:198] conv2 needs backward computation.\r\n",
      "I0124 08:17:32.515012   108 net.cpp:198] pool1 needs backward computation.\r\n",
      "I0124 08:17:32.515029   108 net.cpp:198] relu1 needs backward computation.\r\n",
      "I0124 08:17:32.515043   108 net.cpp:198] conv1 needs backward computation.\r\n",
      "I0124 08:17:32.515056   108 net.cpp:200] data does not need backward computation.\r\n",
      "I0124 08:17:32.515069   108 net.cpp:242] This network produces output loss\r\n",
      "I0124 08:17:32.515098   108 net.cpp:255] Network initialization done.\r\n",
      "I0124 08:17:32.515527   108 solver.cpp:190] Creating test net (#0) specified by net file: caffe-cnn/cnn/cnn.prototxt\r\n",
      "I0124 08:17:32.515625   108 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\r\n",
      "I0124 08:17:32.515862   108 net.cpp:51] Initializing net from parameters: \r\n",
      "name: \"CNN\"\r\n",
      "state {\r\n",
      "  phase: TEST\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    mirror: false\r\n",
      "    crop_size: 128\r\n",
      "    mean_value: 104\r\n",
      "    mean_value: 117\r\n",
      "    mean_value: 123\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./input/validation_lmdb\"\r\n",
      "    batch_size: 32\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 32\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"conv1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 64\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu2\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"conv2\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv3\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"conv3\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 128\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu3\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"conv3\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool3\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"pool3\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv4\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool3\"\r\n",
      "  top: \"conv4\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 128\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu4\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"conv4\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool4\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"pool4\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc5\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool4\"\r\n",
      "  top: \"fc5\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 512\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu5\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc5\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"drop5\"\r\n",
      "  type: \"Dropout\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc5\"\r\n",
      "  dropout_param {\r\n",
      "    dropout_ratio: 0.5\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc6\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc5\"\r\n",
      "  top: \"fc6\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"accuracy\"\r\n",
      "  type: \"Accuracy\"\r\n",
      "  bottom: \"fc6\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"accuracy\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"fc6\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "I0124 08:17:32.518448   108 layer_factory.hpp:77] Creating layer data\r\n",
      "I0124 08:17:32.518682   108 db_lmdb.cpp:35] Opened lmdb ./input/validation_lmdb\r\n",
      "I0124 08:17:32.518730   108 net.cpp:84] Creating Layer data\r\n",
      "I0124 08:17:32.518849   108 net.cpp:380] data -> data\r\n",
      "I0124 08:17:32.518882   108 net.cpp:380] data -> label\r\n",
      "I0124 08:17:32.519048   108 data_layer.cpp:45] output data size: 32,3,128,128\r\n",
      "I0124 08:17:32.519167   108 net.cpp:122] Setting up data\r\n",
      "I0124 08:17:32.519258   108 net.cpp:129] Top shape: 32 3 128 128 (1572864)\r\n",
      "I0124 08:17:32.519275   108 net.cpp:129] Top shape: 32 (32)\r\n",
      "I0124 08:17:32.519289   108 net.cpp:137] Memory required for data: 6291584\r\n",
      "I0124 08:17:32.519304   108 layer_factory.hpp:77] Creating layer label_data_1_split\r\n",
      "I0124 08:17:32.519322   108 net.cpp:84] Creating Layer label_data_1_split\r\n",
      "I0124 08:17:32.519336   108 net.cpp:406] label_data_1_split <- label\r\n",
      "I0124 08:17:32.519354   108 net.cpp:380] label_data_1_split -> label_data_1_split_0\r\n",
      "I0124 08:17:32.519460   108 net.cpp:380] label_data_1_split -> label_data_1_split_1\r\n",
      "I0124 08:17:32.519495   108 net.cpp:122] Setting up label_data_1_split\r\n",
      "I0124 08:17:32.519513   108 net.cpp:129] Top shape: 32 (32)\r\n",
      "I0124 08:17:32.519531   108 net.cpp:129] Top shape: 32 (32)\r\n",
      "I0124 08:17:32.519546   108 net.cpp:137] Memory required for data: 6291840\r\n",
      "I0124 08:17:32.519562   108 layer_factory.hpp:77] Creating layer conv1\r\n",
      "I0124 08:17:32.519585   108 net.cpp:84] Creating Layer conv1\r\n",
      "I0124 08:17:32.519678   108 net.cpp:406] conv1 <- data\r\n",
      "I0124 08:17:32.519703   108 net.cpp:380] conv1 -> conv1\r\n",
      "I0124 08:17:32.519765   108 net.cpp:122] Setting up conv1\r\n",
      "I0124 08:17:32.519791   108 net.cpp:129] Top shape: 32 32 128 128 (16777216)\r\n",
      "I0124 08:17:32.519872   108 net.cpp:137] Memory required for data: 73400704\r\n",
      "I0124 08:17:32.519899   108 layer_factory.hpp:77] Creating layer relu1\r\n",
      "I0124 08:17:32.519917   108 net.cpp:84] Creating Layer relu1\r\n",
      "I0124 08:17:32.519932   108 net.cpp:406] relu1 <- conv1\r\n",
      "I0124 08:17:32.519948   108 net.cpp:367] relu1 -> conv1 (in-place)\r\n",
      "I0124 08:17:32.519965   108 net.cpp:122] Setting up relu1\r\n",
      "I0124 08:17:32.519979   108 net.cpp:129] Top shape: 32 32 128 128 (16777216)\r\n",
      "I0124 08:17:32.519990   108 net.cpp:137] Memory required for data: 140509568\r\n",
      "I0124 08:17:32.520007   108 layer_factory.hpp:77] Creating layer pool1\r\n",
      "I0124 08:17:32.520093   108 net.cpp:84] Creating Layer pool1\r\n",
      "I0124 08:17:32.520112   108 net.cpp:406] pool1 <- conv1\r\n",
      "I0124 08:17:32.520130   108 net.cpp:380] pool1 -> pool1\r\n",
      "I0124 08:17:32.520153   108 net.cpp:122] Setting up pool1\r\n",
      "I0124 08:17:32.520170   108 net.cpp:129] Top shape: 32 32 64 64 (4194304)\r\n",
      "I0124 08:17:32.520182   108 net.cpp:137] Memory required for data: 157286784\r\n",
      "I0124 08:17:32.520195   108 layer_factory.hpp:77] Creating layer conv2\r\n",
      "I0124 08:17:32.520284   108 net.cpp:84] Creating Layer conv2\r\n",
      "I0124 08:17:32.520308   108 net.cpp:406] conv2 <- pool1\r\n",
      "I0124 08:17:32.520326   108 net.cpp:380] conv2 -> conv2\r\n",
      "I0124 08:17:32.520417   108 net.cpp:122] Setting up conv2\r\n",
      "I0124 08:17:32.520511   108 net.cpp:129] Top shape: 32 64 64 64 (8388608)\r\n",
      "I0124 08:17:32.520532   108 net.cpp:137] Memory required for data: 190841216\r\n",
      "I0124 08:17:32.520556   108 layer_factory.hpp:77] Creating layer relu2\r\n",
      "I0124 08:17:32.520576   108 net.cpp:84] Creating Layer relu2\r\n",
      "I0124 08:17:32.520597   108 net.cpp:406] relu2 <- conv2\r\n",
      "I0124 08:17:32.520622   108 net.cpp:367] relu2 -> conv2 (in-place)\r\n",
      "I0124 08:17:32.520717   108 net.cpp:122] Setting up relu2\r\n",
      "I0124 08:17:32.520745   108 net.cpp:129] Top shape: 32 64 64 64 (8388608)\r\n",
      "I0124 08:17:32.520761   108 net.cpp:137] Memory required for data: 224395648\r\n",
      "I0124 08:17:32.520777   108 layer_factory.hpp:77] Creating layer pool2\r\n",
      "I0124 08:17:32.520800   108 net.cpp:84] Creating Layer pool2\r\n",
      "I0124 08:17:32.520817   108 net.cpp:406] pool2 <- conv2\r\n",
      "I0124 08:17:32.520835   108 net.cpp:380] pool2 -> pool2\r\n",
      "I0124 08:17:32.520931   108 net.cpp:122] Setting up pool2\r\n",
      "I0124 08:17:32.520998   108 net.cpp:129] Top shape: 32 64 32 32 (2097152)\r\n",
      "I0124 08:17:32.521013   108 net.cpp:137] Memory required for data: 232784256\r\n",
      "I0124 08:17:32.521032   108 layer_factory.hpp:77] Creating layer conv3\r\n",
      "I0124 08:17:32.521067   108 net.cpp:84] Creating Layer conv3\r\n",
      "I0124 08:17:32.521247   108 net.cpp:406] conv3 <- pool2\r\n",
      "I0124 08:17:32.521293   108 net.cpp:380] conv3 -> conv3\r\n",
      "I0124 08:17:32.521953   108 net.cpp:122] Setting up conv3\r\n",
      "I0124 08:17:32.522133   108 net.cpp:129] Top shape: 32 128 32 32 (4194304)\r\n",
      "I0124 08:17:32.522158   108 net.cpp:137] Memory required for data: 249561472\r\n",
      "I0124 08:17:32.522187   108 layer_factory.hpp:77] Creating layer relu3\r\n",
      "I0124 08:17:32.522217   108 net.cpp:84] Creating Layer relu3\r\n",
      "I0124 08:17:32.522236   108 net.cpp:406] relu3 <- conv3\r\n",
      "I0124 08:17:32.522404   108 net.cpp:367] relu3 -> conv3 (in-place)\r\n",
      "I0124 08:17:32.522446   108 net.cpp:122] Setting up relu3\r\n",
      "I0124 08:17:32.522478   108 net.cpp:129] Top shape: 32 128 32 32 (4194304)\r\n",
      "I0124 08:17:32.522500   108 net.cpp:137] Memory required for data: 266338688\r\n",
      "I0124 08:17:32.522617   108 layer_factory.hpp:77] Creating layer pool3\r\n",
      "I0124 08:17:32.522642   108 net.cpp:84] Creating Layer pool3\r\n",
      "I0124 08:17:32.522657   108 net.cpp:406] pool3 <- conv3\r\n",
      "I0124 08:17:32.522676   108 net.cpp:380] pool3 -> pool3\r\n",
      "I0124 08:17:32.522707   108 net.cpp:122] Setting up pool3\r\n",
      "I0124 08:17:32.522725   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.522740   108 net.cpp:137] Memory required for data: 270532992\r\n",
      "I0124 08:17:32.522855   108 layer_factory.hpp:77] Creating layer conv4\r\n",
      "I0124 08:17:32.522889   108 net.cpp:84] Creating Layer conv4\r\n",
      "I0124 08:17:32.522907   108 net.cpp:406] conv4 <- pool3\r\n",
      "I0124 08:17:32.522927   108 net.cpp:380] conv4 -> conv4\r\n",
      "I0124 08:17:32.523547   108 net.cpp:122] Setting up conv4\r\n",
      "I0124 08:17:32.523699   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.523721   108 net.cpp:137] Memory required for data: 274727296\r\n",
      "I0124 08:17:32.523749   108 layer_factory.hpp:77] Creating layer relu4\r\n",
      "I0124 08:17:32.523779   108 net.cpp:84] Creating Layer relu4\r\n",
      "I0124 08:17:32.523800   108 net.cpp:406] relu4 <- conv4\r\n",
      "I0124 08:17:32.523938   108 net.cpp:367] relu4 -> conv4 (in-place)\r\n",
      "I0124 08:17:32.523972   108 net.cpp:122] Setting up relu4\r\n",
      "I0124 08:17:32.523993   108 net.cpp:129] Top shape: 32 128 16 16 (1048576)\r\n",
      "I0124 08:17:32.524009   108 net.cpp:137] Memory required for data: 278921600\r\n",
      "I0124 08:17:32.524025   108 layer_factory.hpp:77] Creating layer pool4\r\n",
      "I0124 08:17:32.524045   108 net.cpp:84] Creating Layer pool4\r\n",
      "I0124 08:17:32.524062   108 net.cpp:406] pool4 <- conv4\r\n",
      "I0124 08:17:32.524159   108 net.cpp:380] pool4 -> pool4\r\n",
      "I0124 08:17:32.524191   108 net.cpp:122] Setting up pool4\r\n",
      "I0124 08:17:32.524212   108 net.cpp:129] Top shape: 32 128 8 8 (262144)\r\n",
      "I0124 08:17:32.524230   108 net.cpp:137] Memory required for data: 279970176\r\n",
      "I0124 08:17:32.524245   108 layer_factory.hpp:77] Creating layer fc5\r\n",
      "I0124 08:17:32.524272   108 net.cpp:84] Creating Layer fc5\r\n",
      "I0124 08:17:32.524355   108 net.cpp:406] fc5 <- pool4\r\n",
      "I0124 08:17:32.524385   108 net.cpp:380] fc5 -> fc5\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:17:32.579360   108 net.cpp:122] Setting up fc5\n",
      "I0124 08:17:32.579392   108 net.cpp:129] Top shape: 32 512 (16384)\n",
      "I0124 08:17:32.579397   108 net.cpp:137] Memory required for data: 280035712\n",
      "I0124 08:17:32.579411   108 layer_factory.hpp:77] Creating layer relu5\n",
      "I0124 08:17:32.579421   108 net.cpp:84] Creating Layer relu5\n",
      "I0124 08:17:32.579428   108 net.cpp:406] relu5 <- fc5\n",
      "I0124 08:17:32.579437   108 net.cpp:367] relu5 -> fc5 (in-place)\n",
      "I0124 08:17:32.579447   108 net.cpp:122] Setting up relu5\n",
      "I0124 08:17:32.579452   108 net.cpp:129] Top shape: 32 512 (16384)\n",
      "I0124 08:17:32.579455   108 net.cpp:137] Memory required for data: 280101248\n",
      "I0124 08:17:32.579459   108 layer_factory.hpp:77] Creating layer drop5\n",
      "I0124 08:17:32.579470   108 net.cpp:84] Creating Layer drop5\n",
      "I0124 08:17:32.579475   108 net.cpp:406] drop5 <- fc5\n",
      "I0124 08:17:32.579480   108 net.cpp:367] drop5 -> fc5 (in-place)\n",
      "I0124 08:17:32.579488   108 net.cpp:122] Setting up drop5\n",
      "I0124 08:17:32.579522   108 net.cpp:129] Top shape: 32 512 (16384)\n",
      "I0124 08:17:32.579527   108 net.cpp:137] Memory required for data: 280166784\n",
      "I0124 08:17:32.579531   108 layer_factory.hpp:77] Creating layer fc6\n",
      "I0124 08:17:32.579537   108 net.cpp:84] Creating Layer fc6\n",
      "I0124 08:17:32.579542   108 net.cpp:406] fc6 <- fc5\n",
      "I0124 08:17:32.579548   108 net.cpp:380] fc6 -> fc6\n",
      "I0124 08:17:32.579564   108 net.cpp:122] Setting up fc6\n",
      "I0124 08:17:32.579571   108 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:17:32.579576   108 net.cpp:137] Memory required for data: 280168064\n",
      "I0124 08:17:32.579583   108 layer_factory.hpp:77] Creating layer fc6_fc6_0_split\n",
      "I0124 08:17:32.579593   108 net.cpp:84] Creating Layer fc6_fc6_0_split\n",
      "I0124 08:17:32.579598   108 net.cpp:406] fc6_fc6_0_split <- fc6\n",
      "I0124 08:17:32.579604   108 net.cpp:380] fc6_fc6_0_split -> fc6_fc6_0_split_0\n",
      "I0124 08:17:32.579613   108 net.cpp:380] fc6_fc6_0_split -> fc6_fc6_0_split_1\n",
      "I0124 08:17:32.579622   108 net.cpp:122] Setting up fc6_fc6_0_split\n",
      "I0124 08:17:32.579627   108 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:17:32.579632   108 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:17:32.579638   108 net.cpp:137] Memory required for data: 280170624\n",
      "I0124 08:17:32.579643   108 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0124 08:17:32.579649   108 net.cpp:84] Creating Layer accuracy\n",
      "I0124 08:17:32.579654   108 net.cpp:406] accuracy <- fc6_fc6_0_split_0\n",
      "I0124 08:17:32.579661   108 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0124 08:17:32.579681   108 net.cpp:380] accuracy -> accuracy\n",
      "I0124 08:17:32.579692   108 net.cpp:122] Setting up accuracy\n",
      "I0124 08:17:32.579699   108 net.cpp:129] Top shape: (1)\n",
      "I0124 08:17:32.579704   108 net.cpp:137] Memory required for data: 280170628\n",
      "I0124 08:17:32.579708   108 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:17:32.579715   108 net.cpp:84] Creating Layer loss\n",
      "I0124 08:17:32.579720   108 net.cpp:406] loss <- fc6_fc6_0_split_1\n",
      "I0124 08:17:32.579726   108 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0124 08:17:32.579738   108 net.cpp:380] loss -> loss\n",
      "I0124 08:17:32.579747   108 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:17:32.579762   108 net.cpp:122] Setting up loss\n",
      "I0124 08:17:32.579769   108 net.cpp:129] Top shape: (1)\n",
      "I0124 08:17:32.579774   108 net.cpp:132]     with loss weight 1\n",
      "I0124 08:17:32.579785   108 net.cpp:137] Memory required for data: 280170632\n",
      "I0124 08:17:32.579789   108 net.cpp:198] loss needs backward computation.\n",
      "I0124 08:17:32.579797   108 net.cpp:200] accuracy does not need backward computation.\n",
      "I0124 08:17:32.579802   108 net.cpp:198] fc6_fc6_0_split needs backward computation.\n",
      "I0124 08:17:32.579807   108 net.cpp:198] fc6 needs backward computation.\n",
      "I0124 08:17:32.579813   108 net.cpp:198] drop5 needs backward computation.\n",
      "I0124 08:17:32.579818   108 net.cpp:198] relu5 needs backward computation.\n",
      "I0124 08:17:32.579823   108 net.cpp:198] fc5 needs backward computation.\n",
      "I0124 08:17:32.579828   108 net.cpp:198] pool4 needs backward computation.\n",
      "I0124 08:17:32.579833   108 net.cpp:198] relu4 needs backward computation.\n",
      "I0124 08:17:32.579838   108 net.cpp:198] conv4 needs backward computation.\n",
      "I0124 08:17:32.579843   108 net.cpp:198] pool3 needs backward computation.\n",
      "I0124 08:17:32.579849   108 net.cpp:198] relu3 needs backward computation.\n",
      "I0124 08:17:32.579854   108 net.cpp:198] conv3 needs backward computation.\n",
      "I0124 08:17:32.579860   108 net.cpp:198] pool2 needs backward computation.\n",
      "I0124 08:17:32.579866   108 net.cpp:198] relu2 needs backward computation.\n",
      "I0124 08:17:32.579871   108 net.cpp:198] conv2 needs backward computation.\n",
      "I0124 08:17:32.579876   108 net.cpp:198] pool1 needs backward computation.\n",
      "I0124 08:17:32.579882   108 net.cpp:198] relu1 needs backward computation.\n",
      "I0124 08:17:32.579887   108 net.cpp:198] conv1 needs backward computation.\n",
      "I0124 08:17:32.579892   108 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0124 08:17:32.579900   108 net.cpp:200] data does not need backward computation.\n",
      "I0124 08:17:32.579906   108 net.cpp:242] This network produces output accuracy\n",
      "I0124 08:17:32.579921   108 net.cpp:242] This network produces output loss\n",
      "I0124 08:17:32.579938   108 net.cpp:255] Network initialization done.\n",
      "I0124 08:17:32.579993   108 solver.cpp:57] Solver scaffolding done.\n",
      "I0124 08:17:32.580025   108 caffe.cpp:239] Starting Optimization\n",
      "I0124 08:17:32.580031   108 solver.cpp:293] Solving CNN\n",
      "I0124 08:17:32.580035   108 solver.cpp:294] Learning Rate Policy: step\n",
      "I0124 08:17:32.588950   108 solver.cpp:351] Iteration 0, Testing net (#0)\n",
      "I0124 08:19:25.568037   117 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0124 08:19:28.977738   108 solver.cpp:418]     Test net output #0: accuracy = 0\n",
      "I0124 08:19:28.977932   108 solver.cpp:418]     Test net output #1: loss = 2.30258 (* 1 = 2.30258 loss)\n",
      "I0124 08:19:31.313856   108 solver.cpp:239] Iteration 0 (-1.4013e-45 iter/s, 118.733s/50 iters), loss = 2.30258\n",
      "I0124 08:19:31.313899   108 solver.cpp:258]     Train net output #0: loss = 2.30258 (* 1 = 2.30258 loss)\n",
      "I0124 08:19:31.313913   108 sgd_solver.cpp:112] Iteration 0, lr = 0.001\n",
      "I0124 08:21:19.258452   108 solver.cpp:239] Iteration 50 (0.463203 iter/s, 107.944s/50 iters), loss = 2.32247\n",
      "I0124 08:21:19.258615   108 solver.cpp:258]     Train net output #0: loss = 2.32247 (* 1 = 2.32247 loss)\n",
      "I0124 08:21:19.258625   108 sgd_solver.cpp:112] Iteration 50, lr = 0.001\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./snapshot/cnn\n",
    "!caffe train --solver \"caffe-cnn/cnn/cnn_solver.prototxt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, we need to monitor the loss and the model accuracy. We can stop the process at anytime by pressing stop button. Caffe will take a snapshot of the trained model every 5000 iterations, and store them under `./snapshot/cnn` folder.\n",
    "\n",
    "The snapshots have .caffemodel extension. For example, 5000 iterations snapshot will be called: `cnn_iter_5000.caffemodel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "We will use the trained model to make prediction on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "\n",
    "net = caffe.Net('caffe-cnn/cnn/cnn.prototxt',\n",
    "                './snapshot/cnn/cnn_solver_iter_13.caffemodel', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00 Loss: 2.3903\n"
     ]
    }
   ],
   "source": [
    "out = net.forward()\n",
    "acc, loss = out['accuracy'], out['loss']\n",
    "print('Accuracy: {:.2f} Loss: {:.4f}'.format(acc*100, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_single_image(net, img_path):\n",
    "    \"\"\"Return predicted class on a single image.\n",
    "    \n",
    "    Args\n",
    "    :net: model instance.\n",
    "    :img_path: Path to the image\n",
    "    \n",
    "    Returns\n",
    "    :prob: The output probabilities of classes\n",
    "    \"\"\"\n",
    "    input_shape = list(net.blobs['data'].shape)\n",
    "    h, w = input_shape[-2:]\n",
    "    img = caffe.io.load_image(img_path)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    net.blobs['data'].data[...] = img\n",
    "    out = net.forward()\n",
    "    pred = out['prob'][0]  # Single image\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 5\n"
     ]
    }
   ],
   "source": [
    "import caffe\n",
    "\n",
    "net = caffe.Net('caffe-cnn/cnn/cnn_deploy.prototxt',\n",
    "                './snapshot/cnn/cnn_solver_iter_13.caffemodel', caffe.TEST)\n",
    "\n",
    "img_path = './imgs/test/img_1.jpg'\n",
    "pred = predict_on_single_image(net, img_path)\n",
    "print('Class:', np.argmax(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Caffe comes with a repository that is used by researchers and machine learning practitioners to share their trained models. This library is called Model Zoo.\n",
    "\n",
    "Using this command we download the CaffeNet network structure, trained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or download it manually\n",
    "# !gdown https://drive.google.com/uc?id=1s1pBVYak923NPB7EP1TAaVQn-hj-S2Vx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:30:13.292640   332 caffe.cpp:197] Use CPU.\r\n",
      "I0124 08:30:13.293282   332 solver.cpp:45] Initializing solver from parameters: \r\n",
      "test_iter: 141\r\n",
      "test_interval: 1000\r\n",
      "base_lr: 0.001\r\n",
      "display: 50\r\n",
      "max_iter: 40000\r\n",
      "lr_policy: \"step\"\r\n",
      "gamma: 0.1\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "stepsize: 2500\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"./snapshot/vgg-like\"\r\n",
      "solver_mode: CPU\r\n",
      "net: \"caffe-cnn/vgg-like/net.prototxt\"\r\n",
      "train_state {\r\n",
      "  level: 0\r\n",
      "  stage: \"\"\r\n",
      "}\r\n",
      "weights: \"bvlc_reference_caffenet.caffemodel\"\r\n",
      "I0124 08:30:13.293772   332 solver.cpp:102] Creating training net from net file: caffe-cnn/vgg-like/net.prototxt\r\n",
      "I0124 08:30:13.294454   332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\r\n",
      "I0124 08:30:13.294539   332 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\r\n",
      "I0124 08:30:13.294970   332 net.cpp:51] Initializing net from parameters: \r\n",
      "name: \"CaffeNet\"\r\n",
      "state {\r\n",
      "  phase: TRAIN\r\n",
      "  level: 0\r\n",
      "  stage: \"\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TRAIN\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    mirror: true\r\n",
      "    crop_size: 128\r\n",
      "    mean_value: 104\r\n",
      "    mean_value: 117\r\n",
      "    mean_value: 123\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./input/train_lmdb\"\r\n",
      "    batch_size: 32\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 96\r\n",
      "    kernel_size: 11\r\n",
      "    stride: 4\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"conv1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"norm1\"\r\n",
      "  type: \"LRN\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"norm1\"\r\n",
      "  lrn_param {\r\n",
      "    local_size: 5\r\n",
      "    alpha: 0.0001\r\n",
      "    beta: 0.75\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"norm1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 256\r\n",
      "    pad: 2\r\n",
      "    kernel_size: 5\r\n",
      "    group: 2\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu2\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"conv2\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"norm2\"\r\n",
      "  type: \"LRN\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"norm2\"\r\n",
      "  lrn_param {\r\n",
      "    local_size: 5\r\n",
      "    alpha: 0.0001\r\n",
      "    beta: 0.75\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv3\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"norm2\"\r\n",
      "  top: \"conv3\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 384\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu3\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"conv3\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv4\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"conv3\"\r\n",
      "  top: \"conv4\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 384\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "    group: 2\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu4\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"conv4\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv5\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"conv4\"\r\n",
      "  top: \"conv5\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 256\r\n",
      "    pad: 1\r\n",
      "    kernel_size: 3\r\n",
      "    group: 2\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu5\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"conv5\"\r\n",
      "  top: \"conv5\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool5\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv5\"\r\n",
      "  top: \"pool5\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 3\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc6-new\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool5\"\r\n",
      "  top: \"fc6-new\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 4096\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu6\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc6-new\"\r\n",
      "  top: \"fc6-new\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"drop6\"\r\n",
      "  type: \"Dropout\"\r\n",
      "  bottom: \"fc6-new\"\r\n",
      "  top: \"fc6-new\"\r\n",
      "  dropout_param {\r\n",
      "    dropout_ratio: 0.5\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc7\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc6-new\"\r\n",
      "  top: \"fc7\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 4096\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.005\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu7\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc7\"\r\n",
      "  top: \"fc7\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"drop7\"\r\n",
      "  type: \"Dropout\"\r\n",
      "  bottom: \"fc7\"\r\n",
      "  top: \"fc7\"\r\n",
      "  dropout_param {\r\n",
      "    dropout_ratio: 0.5\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc8-10\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc7\"\r\n",
      "  top: \"fc8-10\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "    decay_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "    decay_mult: 0\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"gaussian\"\r\n",
      "      std: 0.01\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "      value: 0\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"fc8-10\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "I0124 08:30:13.299485   332 layer_factory.hpp:77] Creating layer data\r\n",
      "I0124 08:30:13.299700   332 db_lmdb.cpp:35] Opened lmdb ./input/train_lmdb\r\n",
      "I0124 08:30:13.299818   332 net.cpp:84] Creating Layer data\r\n",
      "I0124 08:30:13.299851   332 net.cpp:380] data -> data\r\n",
      "I0124 08:30:13.299891   332 net.cpp:380] data -> label\r\n",
      "I0124 08:30:13.300305   332 data_layer.cpp:45] output data size: 32,3,128,128\r\n",
      "I0124 08:30:13.300451   332 net.cpp:122] Setting up data\r\n",
      "I0124 08:30:13.300482   332 net.cpp:129] Top shape: 32 3 128 128 (1572864)\r\n",
      "I0124 08:30:13.300570   332 net.cpp:129] Top shape: 32 (32)\r\n",
      "I0124 08:30:13.300593   332 net.cpp:137] Memory required for data: 6291584\r\n",
      "I0124 08:30:13.300616   332 layer_factory.hpp:77] Creating layer conv1\r\n",
      "I0124 08:30:13.300645   332 net.cpp:84] Creating Layer conv1\r\n",
      "I0124 08:30:13.300665   332 net.cpp:406] conv1 <- data\r\n",
      "I0124 08:30:13.300690   332 net.cpp:380] conv1 -> conv1\r\n",
      "I0124 08:30:13.301693   332 net.cpp:122] Setting up conv1\r\n",
      "I0124 08:30:13.301762   332 net.cpp:129] Top shape: 32 96 30 30 (2764800)\r\n",
      "I0124 08:30:13.301784   332 net.cpp:137] Memory required for data: 17350784\r\n",
      "I0124 08:30:13.301813   332 layer_factory.hpp:77] Creating layer relu1\r\n",
      "I0124 08:30:13.301834   332 net.cpp:84] Creating Layer relu1\r\n",
      "I0124 08:30:13.301853   332 net.cpp:406] relu1 <- conv1\r\n",
      "I0124 08:30:13.301872   332 net.cpp:367] relu1 -> conv1 (in-place)\r\n",
      "I0124 08:30:13.301964   332 net.cpp:122] Setting up relu1\r\n",
      "I0124 08:30:13.301992   332 net.cpp:129] Top shape: 32 96 30 30 (2764800)\r\n",
      "I0124 08:30:13.302011   332 net.cpp:137] Memory required for data: 28409984\r\n",
      "I0124 08:30:13.302027   332 layer_factory.hpp:77] Creating layer pool1\r\n",
      "I0124 08:30:13.302047   332 net.cpp:84] Creating Layer pool1\r\n",
      "I0124 08:30:13.302063   332 net.cpp:406] pool1 <- conv1\r\n",
      "I0124 08:30:13.302083   332 net.cpp:380] pool1 -> pool1\r\n",
      "I0124 08:30:13.302181   332 net.cpp:122] Setting up pool1\r\n",
      "I0124 08:30:13.302211   332 net.cpp:129] Top shape: 32 96 15 15 (691200)\r\n",
      "I0124 08:30:13.302227   332 net.cpp:137] Memory required for data: 31174784\r\n",
      "I0124 08:30:13.302245   332 layer_factory.hpp:77] Creating layer norm1\r\n",
      "I0124 08:30:13.302266   332 net.cpp:84] Creating Layer norm1\r\n",
      "I0124 08:30:13.302284   332 net.cpp:406] norm1 <- pool1\r\n",
      "I0124 08:30:13.302302   332 net.cpp:380] norm1 -> norm1\r\n",
      "I0124 08:30:13.302403   332 net.cpp:122] Setting up norm1\r\n",
      "I0124 08:30:13.302425   332 net.cpp:129] Top shape: 32 96 15 15 (691200)\r\n",
      "I0124 08:30:13.302461   332 net.cpp:137] Memory required for data: 33939584\r\n",
      "I0124 08:30:13.302479   332 layer_factory.hpp:77] Creating layer conv2\r\n",
      "I0124 08:30:13.302502   332 net.cpp:84] Creating Layer conv2\r\n",
      "I0124 08:30:13.302590   332 net.cpp:406] conv2 <- norm1\r\n",
      "I0124 08:30:13.302618   332 net.cpp:380] conv2 -> conv2\r\n",
      "I0124 08:30:13.309918   332 net.cpp:122] Setting up conv2\r\n",
      "I0124 08:30:13.309988   332 net.cpp:129] Top shape: 32 256 15 15 (1843200)\r\n",
      "I0124 08:30:13.310009   332 net.cpp:137] Memory required for data: 41312384\r\n",
      "I0124 08:30:13.310032   332 layer_factory.hpp:77] Creating layer relu2\r\n",
      "I0124 08:30:13.310052   332 net.cpp:84] Creating Layer relu2\r\n",
      "I0124 08:30:13.310070   332 net.cpp:406] relu2 <- conv2\r\n",
      "I0124 08:30:13.310088   332 net.cpp:367] relu2 -> conv2 (in-place)\r\n",
      "I0124 08:30:13.310108   332 net.cpp:122] Setting up relu2\r\n",
      "I0124 08:30:13.310199   332 net.cpp:129] Top shape: 32 256 15 15 (1843200)\r\n",
      "I0124 08:30:13.310220   332 net.cpp:137] Memory required for data: 48685184\r\n",
      "I0124 08:30:13.310236   332 layer_factory.hpp:77] Creating layer pool2\r\n",
      "I0124 08:30:13.310256   332 net.cpp:84] Creating Layer pool2\r\n",
      "I0124 08:30:13.310271   332 net.cpp:406] pool2 <- conv2\r\n",
      "I0124 08:30:13.310292   332 net.cpp:380] pool2 -> pool2\r\n",
      "I0124 08:30:13.310314   332 net.cpp:122] Setting up pool2\r\n",
      "I0124 08:30:13.310400   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\r\n",
      "I0124 08:30:13.310423   332 net.cpp:137] Memory required for data: 50290816\r\n",
      "I0124 08:30:13.310441   332 layer_factory.hpp:77] Creating layer norm2\r\n",
      "I0124 08:30:13.310461   332 net.cpp:84] Creating Layer norm2\r\n",
      "I0124 08:30:13.310477   332 net.cpp:406] norm2 <- pool2\r\n",
      "I0124 08:30:13.310497   332 net.cpp:380] norm2 -> norm2\r\n",
      "I0124 08:30:13.310520   332 net.cpp:122] Setting up norm2\r\n",
      "I0124 08:30:13.310604   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\r\n",
      "I0124 08:30:13.310627   332 net.cpp:137] Memory required for data: 51896448\r\n",
      "I0124 08:30:13.310643   332 layer_factory.hpp:77] Creating layer conv3\r\n",
      "I0124 08:30:13.310667   332 net.cpp:84] Creating Layer conv3\r\n",
      "I0124 08:30:13.310683   332 net.cpp:406] conv3 <- norm2\r\n",
      "I0124 08:30:13.310703   332 net.cpp:380] conv3 -> conv3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:30:13.347113   332 net.cpp:122] Setting up conv3\n",
      "I0124 08:30:13.347169   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:13.347185   332 net.cpp:137] Memory required for data: 54304896\n",
      "I0124 08:30:13.347213   332 layer_factory.hpp:77] Creating layer relu3\n",
      "I0124 08:30:13.347234   332 net.cpp:84] Creating Layer relu3\n",
      "I0124 08:30:13.347252   332 net.cpp:406] relu3 <- conv3\n",
      "I0124 08:30:13.347270   332 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0124 08:30:13.347290   332 net.cpp:122] Setting up relu3\n",
      "I0124 08:30:13.347306   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:13.347321   332 net.cpp:137] Memory required for data: 56713344\n",
      "I0124 08:30:13.347337   332 layer_factory.hpp:77] Creating layer conv4\n",
      "I0124 08:30:13.347358   332 net.cpp:84] Creating Layer conv4\n",
      "I0124 08:30:13.347373   332 net.cpp:406] conv4 <- conv3\n",
      "I0124 08:30:13.347390   332 net.cpp:380] conv4 -> conv4\n",
      "I0124 08:30:13.361053   332 net.cpp:122] Setting up conv4\n",
      "I0124 08:30:13.368619   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:13.368644   332 net.cpp:137] Memory required for data: 59121792\n",
      "I0124 08:30:13.368672   332 layer_factory.hpp:77] Creating layer relu4\n",
      "I0124 08:30:13.368702   332 net.cpp:84] Creating Layer relu4\n",
      "I0124 08:30:13.368726   332 net.cpp:406] relu4 <- conv4\n",
      "I0124 08:30:13.368752   332 net.cpp:367] relu4 -> conv4 (in-place)\n",
      "I0124 08:30:13.368774   332 net.cpp:122] Setting up relu4\n",
      "I0124 08:30:13.368790   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:13.368803   332 net.cpp:137] Memory required for data: 61530240\n",
      "I0124 08:30:13.368815   332 layer_factory.hpp:77] Creating layer conv5\n",
      "I0124 08:30:13.368837   332 net.cpp:84] Creating Layer conv5\n",
      "I0124 08:30:13.368851   332 net.cpp:406] conv5 <- conv4\n",
      "I0124 08:30:13.368868   332 net.cpp:380] conv5 -> conv5\n",
      "I0124 08:30:13.378080   332 net.cpp:122] Setting up conv5\n",
      "I0124 08:30:13.378152   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:13.378212   332 net.cpp:137] Memory required for data: 63135872\n",
      "I0124 08:30:13.378242   332 layer_factory.hpp:77] Creating layer relu5\n",
      "I0124 08:30:13.378264   332 net.cpp:84] Creating Layer relu5\n",
      "I0124 08:30:13.378283   332 net.cpp:406] relu5 <- conv5\n",
      "I0124 08:30:13.378301   332 net.cpp:367] relu5 -> conv5 (in-place)\n",
      "I0124 08:30:13.378324   332 net.cpp:122] Setting up relu5\n",
      "I0124 08:30:13.378341   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:13.378357   332 net.cpp:137] Memory required for data: 64741504\n",
      "I0124 08:30:13.378371   332 layer_factory.hpp:77] Creating layer pool5\n",
      "I0124 08:30:13.378389   332 net.cpp:84] Creating Layer pool5\n",
      "I0124 08:30:13.378404   332 net.cpp:406] pool5 <- conv5\n",
      "I0124 08:30:13.378423   332 net.cpp:380] pool5 -> pool5\n",
      "I0124 08:30:13.378446   332 net.cpp:122] Setting up pool5\n",
      "I0124 08:30:13.378464   332 net.cpp:129] Top shape: 32 256 3 3 (73728)\n",
      "I0124 08:30:13.378479   332 net.cpp:137] Memory required for data: 65036416\n",
      "I0124 08:30:13.378494   332 layer_factory.hpp:77] Creating layer fc6-new\n",
      "I0124 08:30:13.378516   332 net.cpp:84] Creating Layer fc6-new\n",
      "I0124 08:30:13.378533   332 net.cpp:406] fc6-new <- pool5\n",
      "I0124 08:30:13.378552   332 net.cpp:380] fc6-new -> fc6-new\n",
      "I0124 08:30:13.527719   332 net.cpp:122] Setting up fc6-new\n",
      "I0124 08:30:13.527748   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.527752   332 net.cpp:137] Memory required for data: 65560704\n",
      "I0124 08:30:13.527760   332 layer_factory.hpp:77] Creating layer relu6\n",
      "I0124 08:30:13.527770   332 net.cpp:84] Creating Layer relu6\n",
      "I0124 08:30:13.527773   332 net.cpp:406] relu6 <- fc6-new\n",
      "I0124 08:30:13.527779   332 net.cpp:367] relu6 -> fc6-new (in-place)\n",
      "I0124 08:30:13.527791   332 net.cpp:122] Setting up relu6\n",
      "I0124 08:30:13.527796   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.527799   332 net.cpp:137] Memory required for data: 66084992\n",
      "I0124 08:30:13.527802   332 layer_factory.hpp:77] Creating layer drop6\n",
      "I0124 08:30:13.527807   332 net.cpp:84] Creating Layer drop6\n",
      "I0124 08:30:13.527812   332 net.cpp:406] drop6 <- fc6-new\n",
      "I0124 08:30:13.527815   332 net.cpp:367] drop6 -> fc6-new (in-place)\n",
      "I0124 08:30:13.527824   332 net.cpp:122] Setting up drop6\n",
      "I0124 08:30:13.527828   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.527832   332 net.cpp:137] Memory required for data: 66609280\n",
      "I0124 08:30:13.527835   332 layer_factory.hpp:77] Creating layer fc7\n",
      "I0124 08:30:13.527842   332 net.cpp:84] Creating Layer fc7\n",
      "I0124 08:30:13.527846   332 net.cpp:406] fc7 <- fc6-new\n",
      "I0124 08:30:13.527853   332 net.cpp:380] fc7 -> fc7\n",
      "I0124 08:30:13.790294   332 net.cpp:122] Setting up fc7\n",
      "I0124 08:30:13.790349   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.790356   332 net.cpp:137] Memory required for data: 67133568\n",
      "I0124 08:30:13.790369   332 layer_factory.hpp:77] Creating layer relu7\n",
      "I0124 08:30:13.790378   332 net.cpp:84] Creating Layer relu7\n",
      "I0124 08:30:13.790385   332 net.cpp:406] relu7 <- fc7\n",
      "I0124 08:30:13.790395   332 net.cpp:367] relu7 -> fc7 (in-place)\n",
      "I0124 08:30:13.790405   332 net.cpp:122] Setting up relu7\n",
      "I0124 08:30:13.790411   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.790416   332 net.cpp:137] Memory required for data: 67657856\n",
      "I0124 08:30:13.790421   332 layer_factory.hpp:77] Creating layer drop7\n",
      "I0124 08:30:13.790427   332 net.cpp:84] Creating Layer drop7\n",
      "I0124 08:30:13.790432   332 net.cpp:406] drop7 <- fc7\n",
      "I0124 08:30:13.790438   332 net.cpp:367] drop7 -> fc7 (in-place)\n",
      "I0124 08:30:13.790446   332 net.cpp:122] Setting up drop7\n",
      "I0124 08:30:13.790452   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:13.790457   332 net.cpp:137] Memory required for data: 68182144\n",
      "I0124 08:30:13.790462   332 layer_factory.hpp:77] Creating layer fc8-10\n",
      "I0124 08:30:13.790472   332 net.cpp:84] Creating Layer fc8-10\n",
      "I0124 08:30:13.790477   332 net.cpp:406] fc8-10 <- fc7\n",
      "I0124 08:30:13.790483   332 net.cpp:380] fc8-10 -> fc8-10\n",
      "I0124 08:30:13.791201   332 net.cpp:122] Setting up fc8-10\n",
      "I0124 08:30:13.791214   332 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:30:13.791247   332 net.cpp:137] Memory required for data: 68183424\n",
      "I0124 08:30:13.791255   332 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:30:13.791267   332 net.cpp:84] Creating Layer loss\n",
      "I0124 08:30:13.791273   332 net.cpp:406] loss <- fc8-10\n",
      "I0124 08:30:13.791280   332 net.cpp:406] loss <- label\n",
      "I0124 08:30:13.791288   332 net.cpp:380] loss -> loss\n",
      "I0124 08:30:13.791302   332 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:30:13.791324   332 net.cpp:122] Setting up loss\n",
      "I0124 08:30:13.791332   332 net.cpp:129] Top shape: (1)\n",
      "I0124 08:30:13.791337   332 net.cpp:132]     with loss weight 1\n",
      "I0124 08:30:13.791354   332 net.cpp:137] Memory required for data: 68183428\n",
      "I0124 08:30:13.791359   332 net.cpp:198] loss needs backward computation.\n",
      "I0124 08:30:13.791368   332 net.cpp:198] fc8-10 needs backward computation.\n",
      "I0124 08:30:13.791373   332 net.cpp:198] drop7 needs backward computation.\n",
      "I0124 08:30:13.791378   332 net.cpp:198] relu7 needs backward computation.\n",
      "I0124 08:30:13.791383   332 net.cpp:198] fc7 needs backward computation.\n",
      "I0124 08:30:13.791388   332 net.cpp:198] drop6 needs backward computation.\n",
      "I0124 08:30:13.791393   332 net.cpp:198] relu6 needs backward computation.\n",
      "I0124 08:30:13.791396   332 net.cpp:198] fc6-new needs backward computation.\n",
      "I0124 08:30:13.791401   332 net.cpp:198] pool5 needs backward computation.\n",
      "I0124 08:30:13.791407   332 net.cpp:198] relu5 needs backward computation.\n",
      "I0124 08:30:13.791412   332 net.cpp:198] conv5 needs backward computation.\n",
      "I0124 08:30:13.791416   332 net.cpp:198] relu4 needs backward computation.\n",
      "I0124 08:30:13.791421   332 net.cpp:198] conv4 needs backward computation.\n",
      "I0124 08:30:13.791426   332 net.cpp:198] relu3 needs backward computation.\n",
      "I0124 08:30:13.791431   332 net.cpp:198] conv3 needs backward computation.\n",
      "I0124 08:30:13.791436   332 net.cpp:198] norm2 needs backward computation.\n",
      "I0124 08:30:13.791441   332 net.cpp:198] pool2 needs backward computation.\n",
      "I0124 08:30:13.791446   332 net.cpp:198] relu2 needs backward computation.\n",
      "I0124 08:30:13.791451   332 net.cpp:198] conv2 needs backward computation.\n",
      "I0124 08:30:13.791455   332 net.cpp:198] norm1 needs backward computation.\n",
      "I0124 08:30:13.791460   332 net.cpp:198] pool1 needs backward computation.\n",
      "I0124 08:30:13.791466   332 net.cpp:198] relu1 needs backward computation.\n",
      "I0124 08:30:13.791471   332 net.cpp:198] conv1 needs backward computation.\n",
      "I0124 08:30:13.791476   332 net.cpp:200] data does not need backward computation.\n",
      "I0124 08:30:13.791481   332 net.cpp:242] This network produces output loss\n",
      "I0124 08:30:13.791497   332 net.cpp:255] Network initialization done.\n",
      "I0124 08:30:13.791558   332 solver.cpp:72] Finetuning from bvlc_reference_caffenet.caffemodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:30:13.987154   332 upgrade_proto.cpp:46] Attempting to upgrade input file specified using deprecated transformation parameters: bvlc_reference_caffenet.caffemodel\n",
      "I0124 08:30:13.987182   332 upgrade_proto.cpp:49] Successfully upgraded file specified using deprecated data transformation parameters.\n",
      "W0124 08:30:13.987190   332 upgrade_proto.cpp:51] Note that future Caffe releases will only support transform_param messages for transformation fields.\n",
      "I0124 08:30:13.987419   332 upgrade_proto.cpp:55] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_caffenet.caffemodel\n",
      "I0124 08:30:14.264775   332 upgrade_proto.cpp:63] Successfully upgraded file specified using deprecated V1LayerParameter\n",
      "I0124 08:30:14.277973   332 net.cpp:744] Ignoring source layer fc6\n",
      "I0124 08:30:14.286594   332 net.cpp:744] Ignoring source layer fc8\n",
      "I0124 08:30:14.317555   332 solver.cpp:190] Creating test net (#0) specified by net file: caffe-cnn/vgg-like/net.prototxt\n",
      "I0124 08:30:14.317620   332 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0124 08:30:14.317803   332 net.cpp:51] Initializing net from parameters: \n",
      "name: \"CaffeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    mirror: false\n",
      "    crop_size: 128\n",
      "    mean_value: 104\n",
      "    mean_value: 117\n",
      "    mean_value: 123\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./input/validation_lmdb\"\n",
      "    batch_size: 32\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6-new\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6-new\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6-new\"\n",
      "  top: \"fc6-new\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6-new\"\n",
      "  top: \"fc6-new\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6-new\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8-10\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8-10\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"fc8-10\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"fc8-10\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0124 08:30:14.318150   332 layer_factory.hpp:77] Creating layer data\n",
      "I0124 08:30:14.318246   332 db_lmdb.cpp:35] Opened lmdb ./input/validation_lmdb\n",
      "I0124 08:30:14.318271   332 net.cpp:84] Creating Layer data\n",
      "I0124 08:30:14.318281   332 net.cpp:380] data -> data\n",
      "I0124 08:30:14.318295   332 net.cpp:380] data -> label\n",
      "I0124 08:30:14.318328   332 data_layer.cpp:45] output data size: 32,3,128,128\n",
      "I0124 08:30:14.318661   332 net.cpp:122] Setting up data\n",
      "I0124 08:30:14.318675   332 net.cpp:129] Top shape: 32 3 128 128 (1572864)\n",
      "I0124 08:30:14.318682   332 net.cpp:129] Top shape: 32 (32)\n",
      "I0124 08:30:14.318688   332 net.cpp:137] Memory required for data: 6291584\n",
      "I0124 08:30:14.318696   332 layer_factory.hpp:77] Creating layer label_data_1_split\n",
      "I0124 08:30:14.318706   332 net.cpp:84] Creating Layer label_data_1_split\n",
      "I0124 08:30:14.318725   332 net.cpp:406] label_data_1_split <- label\n",
      "I0124 08:30:14.318737   332 net.cpp:380] label_data_1_split -> label_data_1_split_0\n",
      "I0124 08:30:14.318747   332 net.cpp:380] label_data_1_split -> label_data_1_split_1\n",
      "I0124 08:30:14.318759   332 net.cpp:122] Setting up label_data_1_split\n",
      "I0124 08:30:14.318768   332 net.cpp:129] Top shape: 32 (32)\n",
      "I0124 08:30:14.318774   332 net.cpp:129] Top shape: 32 (32)\n",
      "I0124 08:30:14.318780   332 net.cpp:137] Memory required for data: 6291840\n",
      "I0124 08:30:14.318796   332 layer_factory.hpp:77] Creating layer conv1\n",
      "I0124 08:30:14.318812   332 net.cpp:84] Creating Layer conv1\n",
      "I0124 08:30:14.318819   332 net.cpp:406] conv1 <- data\n",
      "I0124 08:30:14.318828   332 net.cpp:380] conv1 -> conv1\n",
      "I0124 08:30:14.319314   332 net.cpp:122] Setting up conv1\n",
      "I0124 08:30:14.319332   332 net.cpp:129] Top shape: 32 96 30 30 (2764800)\n",
      "I0124 08:30:14.319339   332 net.cpp:137] Memory required for data: 17351040\n",
      "I0124 08:30:14.319350   332 layer_factory.hpp:77] Creating layer relu1\n",
      "I0124 08:30:14.319358   332 net.cpp:84] Creating Layer relu1\n",
      "I0124 08:30:14.319365   332 net.cpp:406] relu1 <- conv1\n",
      "I0124 08:30:14.319375   332 net.cpp:367] relu1 -> conv1 (in-place)\n",
      "I0124 08:30:14.319383   332 net.cpp:122] Setting up relu1\n",
      "I0124 08:30:14.319391   332 net.cpp:129] Top shape: 32 96 30 30 (2764800)\n",
      "I0124 08:30:14.319397   332 net.cpp:137] Memory required for data: 28410240\n",
      "I0124 08:30:14.319402   332 layer_factory.hpp:77] Creating layer pool1\n",
      "I0124 08:30:14.319411   332 net.cpp:84] Creating Layer pool1\n",
      "I0124 08:30:14.319417   332 net.cpp:406] pool1 <- conv1\n",
      "I0124 08:30:14.319425   332 net.cpp:380] pool1 -> pool1\n",
      "I0124 08:30:14.319439   332 net.cpp:122] Setting up pool1\n",
      "I0124 08:30:14.319449   332 net.cpp:129] Top shape: 32 96 15 15 (691200)\n",
      "I0124 08:30:14.319455   332 net.cpp:137] Memory required for data: 31175040\n",
      "I0124 08:30:14.319461   332 layer_factory.hpp:77] Creating layer norm1\n",
      "I0124 08:30:14.319469   332 net.cpp:84] Creating Layer norm1\n",
      "I0124 08:30:14.319474   332 net.cpp:406] norm1 <- pool1\n",
      "I0124 08:30:14.319483   332 net.cpp:380] norm1 -> norm1\n",
      "I0124 08:30:14.319494   332 net.cpp:122] Setting up norm1\n",
      "I0124 08:30:14.319500   332 net.cpp:129] Top shape: 32 96 15 15 (691200)\n",
      "I0124 08:30:14.319506   332 net.cpp:137] Memory required for data: 33939840\n",
      "I0124 08:30:14.319511   332 layer_factory.hpp:77] Creating layer conv2\n",
      "I0124 08:30:14.319523   332 net.cpp:84] Creating Layer conv2\n",
      "I0124 08:30:14.319550   332 net.cpp:406] conv2 <- norm1\n",
      "I0124 08:30:14.319561   332 net.cpp:380] conv2 -> conv2\n",
      "I0124 08:30:14.323312   332 net.cpp:122] Setting up conv2\n",
      "I0124 08:30:14.323331   332 net.cpp:129] Top shape: 32 256 15 15 (1843200)\n",
      "I0124 08:30:14.323338   332 net.cpp:137] Memory required for data: 41312640\n",
      "I0124 08:30:14.323348   332 layer_factory.hpp:77] Creating layer relu2\n",
      "I0124 08:30:14.323359   332 net.cpp:84] Creating Layer relu2\n",
      "I0124 08:30:14.323365   332 net.cpp:406] relu2 <- conv2\n",
      "I0124 08:30:14.323372   332 net.cpp:367] relu2 -> conv2 (in-place)\n",
      "I0124 08:30:14.323379   332 net.cpp:122] Setting up relu2\n",
      "I0124 08:30:14.323386   332 net.cpp:129] Top shape: 32 256 15 15 (1843200)\n",
      "I0124 08:30:14.323392   332 net.cpp:137] Memory required for data: 48685440\n",
      "I0124 08:30:14.323397   332 layer_factory.hpp:77] Creating layer pool2\n",
      "I0124 08:30:14.323408   332 net.cpp:84] Creating Layer pool2\n",
      "I0124 08:30:14.323415   332 net.cpp:406] pool2 <- conv2\n",
      "I0124 08:30:14.323421   332 net.cpp:380] pool2 -> pool2\n",
      "I0124 08:30:14.323432   332 net.cpp:122] Setting up pool2\n",
      "I0124 08:30:14.323439   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:14.323446   332 net.cpp:137] Memory required for data: 50291072\n",
      "I0124 08:30:14.323451   332 layer_factory.hpp:77] Creating layer norm2\n",
      "I0124 08:30:14.323460   332 net.cpp:84] Creating Layer norm2\n",
      "I0124 08:30:14.323467   332 net.cpp:406] norm2 <- pool2\n",
      "I0124 08:30:14.323474   332 net.cpp:380] norm2 -> norm2\n",
      "I0124 08:30:14.323484   332 net.cpp:122] Setting up norm2\n",
      "I0124 08:30:14.323491   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:14.323498   332 net.cpp:137] Memory required for data: 51896704\n",
      "I0124 08:30:14.323503   332 layer_factory.hpp:77] Creating layer conv3\n",
      "I0124 08:30:14.323514   332 net.cpp:84] Creating Layer conv3\n",
      "I0124 08:30:14.323520   332 net.cpp:406] conv3 <- norm2\n",
      "I0124 08:30:14.323529   332 net.cpp:380] conv3 -> conv3\n",
      "I0124 08:30:14.333500   332 net.cpp:122] Setting up conv3\n",
      "I0124 08:30:14.333529   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:14.333534   332 net.cpp:137] Memory required for data: 54305152\n",
      "I0124 08:30:14.333545   332 layer_factory.hpp:77] Creating layer relu3\n",
      "I0124 08:30:14.333554   332 net.cpp:84] Creating Layer relu3\n",
      "I0124 08:30:14.333560   332 net.cpp:406] relu3 <- conv3\n",
      "I0124 08:30:14.333570   332 net.cpp:367] relu3 -> conv3 (in-place)\n",
      "I0124 08:30:14.333578   332 net.cpp:122] Setting up relu3\n",
      "I0124 08:30:14.333585   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:14.333590   332 net.cpp:137] Memory required for data: 56713600\n",
      "I0124 08:30:14.333593   332 layer_factory.hpp:77] Creating layer conv4\n",
      "I0124 08:30:14.333603   332 net.cpp:84] Creating Layer conv4\n",
      "I0124 08:30:14.333608   332 net.cpp:406] conv4 <- conv3\n",
      "I0124 08:30:14.333616   332 net.cpp:380] conv4 -> conv4\n",
      "I0124 08:30:14.340500   332 net.cpp:122] Setting up conv4\n",
      "I0124 08:30:14.340517   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:14.340523   332 net.cpp:137] Memory required for data: 59122048\n",
      "I0124 08:30:14.340530   332 layer_factory.hpp:77] Creating layer relu4\n",
      "I0124 08:30:14.340538   332 net.cpp:84] Creating Layer relu4\n",
      "I0124 08:30:14.340543   332 net.cpp:406] relu4 <- conv4\n",
      "I0124 08:30:14.340548   332 net.cpp:367] relu4 -> conv4 (in-place)\n",
      "I0124 08:30:14.340557   332 net.cpp:122] Setting up relu4\n",
      "I0124 08:30:14.340564   332 net.cpp:129] Top shape: 32 384 7 7 (602112)\n",
      "I0124 08:30:14.340569   332 net.cpp:137] Memory required for data: 61530496\n",
      "I0124 08:30:14.340572   332 layer_factory.hpp:77] Creating layer conv5\n",
      "I0124 08:30:14.340584   332 net.cpp:84] Creating Layer conv5\n",
      "I0124 08:30:14.340587   332 net.cpp:406] conv5 <- conv4\n",
      "I0124 08:30:14.340595   332 net.cpp:380] conv5 -> conv5\n",
      "I0124 08:30:14.345124   332 net.cpp:122] Setting up conv5\n",
      "I0124 08:30:14.345134   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:14.345139   332 net.cpp:137] Memory required for data: 63136128\n",
      "I0124 08:30:14.345147   332 layer_factory.hpp:77] Creating layer relu5\n",
      "I0124 08:30:14.345155   332 net.cpp:84] Creating Layer relu5\n",
      "I0124 08:30:14.345185   332 net.cpp:406] relu5 <- conv5\n",
      "I0124 08:30:14.345191   332 net.cpp:367] relu5 -> conv5 (in-place)\n",
      "I0124 08:30:14.345197   332 net.cpp:122] Setting up relu5\n",
      "I0124 08:30:14.345202   332 net.cpp:129] Top shape: 32 256 7 7 (401408)\n",
      "I0124 08:30:14.345206   332 net.cpp:137] Memory required for data: 64741760\n",
      "I0124 08:30:14.345209   332 layer_factory.hpp:77] Creating layer pool5\n",
      "I0124 08:30:14.345219   332 net.cpp:84] Creating Layer pool5\n",
      "I0124 08:30:14.345223   332 net.cpp:406] pool5 <- conv5\n",
      "I0124 08:30:14.345229   332 net.cpp:380] pool5 -> pool5\n",
      "I0124 08:30:14.345237   332 net.cpp:122] Setting up pool5\n",
      "I0124 08:30:14.345242   332 net.cpp:129] Top shape: 32 256 3 3 (73728)\n",
      "I0124 08:30:14.345247   332 net.cpp:137] Memory required for data: 65036672\n",
      "I0124 08:30:14.345252   332 layer_factory.hpp:77] Creating layer fc6-new\n",
      "I0124 08:30:14.345261   332 net.cpp:84] Creating Layer fc6-new\n",
      "I0124 08:30:14.345266   332 net.cpp:406] fc6-new <- pool5\n",
      "I0124 08:30:14.345273   332 net.cpp:380] fc6-new -> fc6-new\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:30:14.433934   332 net.cpp:122] Setting up fc6-new\n",
      "I0124 08:30:14.433965   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.433969   332 net.cpp:137] Memory required for data: 65560960\n",
      "I0124 08:30:14.433977   332 layer_factory.hpp:77] Creating layer relu6\n",
      "I0124 08:30:14.433986   332 net.cpp:84] Creating Layer relu6\n",
      "I0124 08:30:14.433991   332 net.cpp:406] relu6 <- fc6-new\n",
      "I0124 08:30:14.433997   332 net.cpp:367] relu6 -> fc6-new (in-place)\n",
      "I0124 08:30:14.434005   332 net.cpp:122] Setting up relu6\n",
      "I0124 08:30:14.434011   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.434015   332 net.cpp:137] Memory required for data: 66085248\n",
      "I0124 08:30:14.434020   332 layer_factory.hpp:77] Creating layer drop6\n",
      "I0124 08:30:14.434026   332 net.cpp:84] Creating Layer drop6\n",
      "I0124 08:30:14.434029   332 net.cpp:406] drop6 <- fc6-new\n",
      "I0124 08:30:14.434033   332 net.cpp:367] drop6 -> fc6-new (in-place)\n",
      "I0124 08:30:14.434039   332 net.cpp:122] Setting up drop6\n",
      "I0124 08:30:14.434043   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.434048   332 net.cpp:137] Memory required for data: 66609536\n",
      "I0124 08:30:14.434052   332 layer_factory.hpp:77] Creating layer fc7\n",
      "I0124 08:30:14.434060   332 net.cpp:84] Creating Layer fc7\n",
      "I0124 08:30:14.434065   332 net.cpp:406] fc7 <- fc6-new\n",
      "I0124 08:30:14.434072   332 net.cpp:380] fc7 -> fc7\n",
      "I0124 08:30:14.583194   332 net.cpp:122] Setting up fc7\n",
      "I0124 08:30:14.583222   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.583226   332 net.cpp:137] Memory required for data: 67133824\n",
      "I0124 08:30:14.583235   332 layer_factory.hpp:77] Creating layer relu7\n",
      "I0124 08:30:14.583243   332 net.cpp:84] Creating Layer relu7\n",
      "I0124 08:30:14.583248   332 net.cpp:406] relu7 <- fc7\n",
      "I0124 08:30:14.583256   332 net.cpp:367] relu7 -> fc7 (in-place)\n",
      "I0124 08:30:14.583266   332 net.cpp:122] Setting up relu7\n",
      "I0124 08:30:14.583271   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.583273   332 net.cpp:137] Memory required for data: 67658112\n",
      "I0124 08:30:14.583276   332 layer_factory.hpp:77] Creating layer drop7\n",
      "I0124 08:30:14.583283   332 net.cpp:84] Creating Layer drop7\n",
      "I0124 08:30:14.583287   332 net.cpp:406] drop7 <- fc7\n",
      "I0124 08:30:14.583290   332 net.cpp:367] drop7 -> fc7 (in-place)\n",
      "I0124 08:30:14.583297   332 net.cpp:122] Setting up drop7\n",
      "I0124 08:30:14.583300   332 net.cpp:129] Top shape: 32 4096 (131072)\n",
      "I0124 08:30:14.583304   332 net.cpp:137] Memory required for data: 68182400\n",
      "I0124 08:30:14.583307   332 layer_factory.hpp:77] Creating layer fc8-10\n",
      "I0124 08:30:14.583314   332 net.cpp:84] Creating Layer fc8-10\n",
      "I0124 08:30:14.583318   332 net.cpp:406] fc8-10 <- fc7\n",
      "I0124 08:30:14.583325   332 net.cpp:380] fc8-10 -> fc8-10\n",
      "I0124 08:30:14.583704   332 net.cpp:122] Setting up fc8-10\n",
      "I0124 08:30:14.583711   332 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:30:14.583715   332 net.cpp:137] Memory required for data: 68183680\n",
      "I0124 08:30:14.583720   332 layer_factory.hpp:77] Creating layer fc8-10_fc8-10_0_split\n",
      "I0124 08:30:14.583725   332 net.cpp:84] Creating Layer fc8-10_fc8-10_0_split\n",
      "I0124 08:30:14.583752   332 net.cpp:406] fc8-10_fc8-10_0_split <- fc8-10\n",
      "I0124 08:30:14.583761   332 net.cpp:380] fc8-10_fc8-10_0_split -> fc8-10_fc8-10_0_split_0\n",
      "I0124 08:30:14.583768   332 net.cpp:380] fc8-10_fc8-10_0_split -> fc8-10_fc8-10_0_split_1\n",
      "I0124 08:30:14.583775   332 net.cpp:122] Setting up fc8-10_fc8-10_0_split\n",
      "I0124 08:30:14.583779   332 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:30:14.583783   332 net.cpp:129] Top shape: 32 10 (320)\n",
      "I0124 08:30:14.583786   332 net.cpp:137] Memory required for data: 68186240\n",
      "I0124 08:30:14.583789   332 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0124 08:30:14.583796   332 net.cpp:84] Creating Layer accuracy\n",
      "I0124 08:30:14.583801   332 net.cpp:406] accuracy <- fc8-10_fc8-10_0_split_0\n",
      "I0124 08:30:14.583806   332 net.cpp:406] accuracy <- label_data_1_split_0\n",
      "I0124 08:30:14.583809   332 net.cpp:380] accuracy -> accuracy\n",
      "I0124 08:30:14.583817   332 net.cpp:122] Setting up accuracy\n",
      "I0124 08:30:14.583822   332 net.cpp:129] Top shape: (1)\n",
      "I0124 08:30:14.583825   332 net.cpp:137] Memory required for data: 68186244\n",
      "I0124 08:30:14.583829   332 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:30:14.583837   332 net.cpp:84] Creating Layer loss\n",
      "I0124 08:30:14.583843   332 net.cpp:406] loss <- fc8-10_fc8-10_0_split_1\n",
      "I0124 08:30:14.583847   332 net.cpp:406] loss <- label_data_1_split_1\n",
      "I0124 08:30:14.583853   332 net.cpp:380] loss -> loss\n",
      "I0124 08:30:14.583860   332 layer_factory.hpp:77] Creating layer loss\n",
      "I0124 08:30:14.583874   332 net.cpp:122] Setting up loss\n",
      "I0124 08:30:14.583880   332 net.cpp:129] Top shape: (1)\n",
      "I0124 08:30:14.583884   332 net.cpp:132]     with loss weight 1\n",
      "I0124 08:30:14.583894   332 net.cpp:137] Memory required for data: 68186248\n",
      "I0124 08:30:14.583899   332 net.cpp:198] loss needs backward computation.\n",
      "I0124 08:30:14.583904   332 net.cpp:200] accuracy does not need backward computation.\n",
      "I0124 08:30:14.583909   332 net.cpp:198] fc8-10_fc8-10_0_split needs backward computation.\n",
      "I0124 08:30:14.583912   332 net.cpp:198] fc8-10 needs backward computation.\n",
      "I0124 08:30:14.583916   332 net.cpp:198] drop7 needs backward computation.\n",
      "I0124 08:30:14.583920   332 net.cpp:198] relu7 needs backward computation.\n",
      "I0124 08:30:14.583925   332 net.cpp:198] fc7 needs backward computation.\n",
      "I0124 08:30:14.583930   332 net.cpp:198] drop6 needs backward computation.\n",
      "I0124 08:30:14.583933   332 net.cpp:198] relu6 needs backward computation.\n",
      "I0124 08:30:14.583937   332 net.cpp:198] fc6-new needs backward computation.\n",
      "I0124 08:30:14.583942   332 net.cpp:198] pool5 needs backward computation.\n",
      "I0124 08:30:14.583946   332 net.cpp:198] relu5 needs backward computation.\n",
      "I0124 08:30:14.583951   332 net.cpp:198] conv5 needs backward computation.\n",
      "I0124 08:30:14.583956   332 net.cpp:198] relu4 needs backward computation.\n",
      "I0124 08:30:14.583959   332 net.cpp:198] conv4 needs backward computation.\n",
      "I0124 08:30:14.583963   332 net.cpp:198] relu3 needs backward computation.\n",
      "I0124 08:30:14.583968   332 net.cpp:198] conv3 needs backward computation.\n",
      "I0124 08:30:14.583972   332 net.cpp:198] norm2 needs backward computation.\n",
      "I0124 08:30:14.583976   332 net.cpp:198] pool2 needs backward computation.\n",
      "I0124 08:30:14.583981   332 net.cpp:198] relu2 needs backward computation.\n",
      "I0124 08:30:14.583986   332 net.cpp:198] conv2 needs backward computation.\n",
      "I0124 08:30:14.583989   332 net.cpp:198] norm1 needs backward computation.\n",
      "I0124 08:30:14.583993   332 net.cpp:198] pool1 needs backward computation.\n",
      "I0124 08:30:14.583998   332 net.cpp:198] relu1 needs backward computation.\n",
      "I0124 08:30:14.584002   332 net.cpp:198] conv1 needs backward computation.\n",
      "I0124 08:30:14.584007   332 net.cpp:200] label_data_1_split does not need backward computation.\n",
      "I0124 08:30:14.584012   332 net.cpp:200] data does not need backward computation.\n",
      "I0124 08:30:14.584017   332 net.cpp:242] This network produces output accuracy\n",
      "I0124 08:30:14.584022   332 net.cpp:242] This network produces output loss\n",
      "I0124 08:30:14.584039   332 net.cpp:255] Network initialization done.\n",
      "I0124 08:30:14.584084   332 solver.cpp:72] Finetuning from bvlc_reference_caffenet.caffemodel\n",
      "I0124 08:30:14.745046   332 upgrade_proto.cpp:46] Attempting to upgrade input file specified using deprecated transformation parameters: bvlc_reference_caffenet.caffemodel\n",
      "I0124 08:30:14.745076   332 upgrade_proto.cpp:49] Successfully upgraded file specified using deprecated data transformation parameters.\n",
      "W0124 08:30:14.745081   332 upgrade_proto.cpp:51] Note that future Caffe releases will only support transform_param messages for transformation fields.\n",
      "I0124 08:30:14.745100   332 upgrade_proto.cpp:55] Attempting to upgrade input file specified using deprecated V1LayerParameter: bvlc_reference_caffenet.caffemodel\n",
      "I0124 08:30:15.013674   332 upgrade_proto.cpp:63] Successfully upgraded file specified using deprecated V1LayerParameter\n",
      "I0124 08:30:15.026810   332 net.cpp:744] Ignoring source layer fc6\n",
      "I0124 08:30:15.035660   332 net.cpp:744] Ignoring source layer fc8\n",
      "I0124 08:30:15.069247   332 solver.cpp:57] Solver scaffolding done.\n",
      "I0124 08:30:15.069344   332 caffe.cpp:239] Starting Optimization\n",
      "I0124 08:30:15.069360   332 solver.cpp:293] Solving CaffeNet\n",
      "I0124 08:30:15.069368   332 solver.cpp:294] Learning Rate Policy: step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0124 08:30:15.120971   332 solver.cpp:351] Iteration 0, Testing net (#0)\n",
      "I0124 08:32:05.797765   341 data_layer.cpp:73] Restarting data prefetching from start.\n",
      "I0124 08:32:09.314066   332 solver.cpp:418]     Test net output #0: accuracy = 0\n",
      "I0124 08:32:09.314137   332 solver.cpp:418]     Test net output #1: loss = 2.30258 (* 1 = 2.30258 loss)\n",
      "I0124 08:32:11.419153   332 solver.cpp:239] Iteration 0 (-1.4013e-45 iter/s, 116.349s/50 iters), loss = 2.30215\n",
      "I0124 08:32:11.419318   332 solver.cpp:258]     Train net output #0: loss = 2.30215 (* 1 = 2.30215 loss)\n",
      "I0124 08:32:11.419355   332 sgd_solver.cpp:112] Iteration 0, lr = 0.001\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!caffe train --solver=\"caffe-cnn/vgg-like/solver.prototxt\" --weights \"bvlc_reference_caffenet.caffemodel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "\n",
    "net = caffe.Net('caffe-cnn/vgg-like/net.prototxt',\n",
    "                './snapshot/vgg-like_iter_36.caffemodel', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.06 Loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "out = net.forward()\n",
    "acc, loss = out['accuracy'], out['loss']\n",
    "print('Accuracy: {:.2f} Loss: {:.2f}'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on single image only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 4\n"
     ]
    }
   ],
   "source": [
    "import caffe\n",
    "\n",
    "net = caffe.Net('caffe-cnn/vgg-like/net_deploy.prototxt',\n",
    "                './snapshot/vgg-like_iter_36.caffemodel', caffe.TEST)\n",
    "\n",
    "img_path = './imgs/test/img_1.jpg'\n",
    "pred = predict_on_single_image(net, img_path)\n",
    "\n",
    "print('Class:', np.argmax(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
